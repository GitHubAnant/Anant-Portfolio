{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Module1: Web scraping.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAo_Tcr-FbmX",
        "colab_type": "code",
        "outputId": "f9570cfd-a2d8-478e-bc20-75db57aa9440",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liuFGynMGpmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import urllib.request\n",
        "import nltk\n",
        "import datetime\n",
        "from urllib.request import urlopen\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    for i, token in enumerate(tokens):\n",
        "        tokens[i] = ''.join([ch for ch in token if ch not in char_blacklist])\n",
        "    tokens_sw = [w.lower() for w in tokens if w not in stopwords]\n",
        "    tokens_lemmatize = [wnl.lemmatize(token) for token in tokens_sw]\n",
        "    return tokens_lemmatize\n",
        "\n",
        "\n",
        "def crawl(object):\n",
        "    print(f\"{object['i']}/{len(urls)} || {object['url']}\")\n",
        "    tokens_lemmatize = ''\n",
        "    try:\n",
        "        req = urllib.request.Request(object['url'], headers=hdr)\n",
        "        html = urlopen(req, timeout=15).read()\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        [tag.decompose() for tag in soup(\"script\")]\n",
        "        [tag.decompose() for tag in soup(\"style\")]\n",
        "        text = soup.get_text()\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        text = '\\n'.join(chunk.lower() for chunk in chunks if chunk)\n",
        "        # Tokenize text\n",
        "        tokens = [token.lower() for token in word_tokenize(text)]\n",
        "        # Remove stopwords\n",
        "        tokens_lemmatize = remove_stopwords(tokens)\n",
        "    except Exception:\n",
        "        print(f\"{object['i']}/{len(urls)} || {object['url']} FAILED. \")\n",
        "    return tokens_lemmatize if len(tokens_lemmatize) else ''\n",
        "    #     return page_tokens[object['i']]\n",
        "\n",
        "date = '2019-10-23'\n",
        "input_path = 'https://drive.google.com/uc?export=download&id=1bSHSVwBF7qrtUM2DXXqZXvc23qp9UW2B'\n",
        "#output_path = f'Datasets/Feature_dataset_{date}.csv'\n",
        "\n",
        "df = pd.read_excel(input_path,header = 0)\n",
        "df.columns = ['id', 'url','target']\n",
        "date = datetime.datetime.today().strftime(\"%Y-%m-%d\")\n",
        "df = df[160000:169900]\n",
        "df.head()\n",
        "hdr = {\n",
        "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "        'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
        "        'Accept-Encoding': 'none',\n",
        "        'Accept-Language': 'en-US,en;q=0.8',\n",
        "        'Connection': 'keep-alive'}\n",
        "wnl = WordNetLemmatizer()\n",
        "char_blacklist = list(\n",
        "        chr(i) for i in range(32, 127) if (i <= 47 or i >= 58) and (i <= 64 or i >= 91) and (i <= 96 or i >= 123))\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stopwords.extend(char_blacklist)\n",
        "\n",
        "start = datetime.datetime.now()\n",
        "print(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKbol0bJG_gT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(start)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hACNdV1WHBUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "urls = [{\"i\": index, \"url\": url} for index, url in enumerate(list(df['url'].values))]\n",
        "print(urls)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDa_kuMrHBsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['tokens'] = ''\n",
        "p = Pool(cpu_count() * 2)\n",
        "tokens = p.map(crawl, urls)\n",
        "df['tokens'][:len(tokens)] = tokens\n",
        "stop = datetime.datetime.now()\n",
        "print(stop)\n",
        "exec_time = stop - start\n",
        "print(exec_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5F9qU7cR-bl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd3FHE8jSTk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "df.to_excel('df160k170k.xlsx',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b7dfQ2XSyzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('df160k170k.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcocauvQS3as",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To Mount\n",
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "# To save on drive\n",
        "!cp df160k170k.xlsx drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}