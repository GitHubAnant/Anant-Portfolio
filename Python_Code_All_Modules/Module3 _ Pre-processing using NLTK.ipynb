{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Module3_Pre_processing_using_NLTK.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"TFAFVLpIz0EI","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","df50k    = pd.read_excel('https://drive.google.com/uc?export=download&id=17IE7hHWmkcVW73CjXL39ufVw2JI60tJl',header = 0)\n","df50k60k = pd.read_excel('https://drive.google.com/uc?export=download&id=1oqAqewWHrG0ojZpQXuVPZKPrSUdFeS69',header = 0)\n","df60k70k = pd.read_excel('https://drive.google.com/uc?export=download&id=13AswabcDHIbQUUPw32EyAQoGeVXnK16i',header = 0)\n","df70k80k = pd.read_excel('https://drive.google.com/uc?export=download&id=1iN24rBHEmeoQlwfwegTc-cVLZeNa5nlA',header = 0)\n","df80k90k = pd.read_excel('https://drive.google.com/uc?export=download&id=1BcS-3eajpnp6AXhLSi7A7Y_patJvRN2Y',header = 0)\n","df90k100k= pd.read_excel('https://drive.google.com/uc?export=download&id=0B-7yvzlCWT6rclF4RVd4dEo3QzJxYm9OVjh1cjNEakd0b3RJ',header = 0)\n","df100k110k = pd.read_excel('https://drive.google.com/uc?export=download&id=1u7I3Earyz0goLJOJvKbV1kHsRgipAp9v',header = 0)\n","df110k120k = pd.read_excel('https://drive.google.com/uc?export=download&id=1_rWNN-IxmmYFidfj1tc8AjW2mKsmk7df',header = 0)\n","df130k140k = pd.read_excel('https://drive.google.com/uc?export=download&id=1G1eumeMzNdXP8Ci3hbPdLHee3zCneo8a',header = 0)\n","df140k150k = pd.read_excel('https://drive.google.com/uc?export=download&id=0B-7yvzlCWT6rY0hoMmhkOVhxbVZxZ0ZIWDNJalFoLXlWV1JF',header = 0)\n","df150k160k = pd.read_excel('https://drive.google.com/uc?export=download&id=153hfDzirFWhec-ZN3xfoBp7Kv2bE5o3q',header = 0)\n","df160k170k = pd.read_excel('https://drive.google.com/uc?export=download&id=14N3xIyE0nQdbmaqLJMGBfpJ72zpMqRBQ',header = 0)\n","df170k180k = pd.read_excel('https://drive.google.com/uc?export=download&id=0B-7yvzlCWT6rUGdTbmFnSlRWWk52dzVpVzl5UWUwWVlZTFhj',header = 0)\n","df180k190k = pd.read_excel('https://drive.google.com/uc?export=download&id=1MBbOlwBNqZDJZYhl0kQ23uyArP_ITj0l',header = 0)\n","df190k200k = pd.read_excel('https://drive.google.com/uc?export=download&id=1SL8a0kHCKtTeXLq3KiYfackWUvRdZdRh',header = 0)\n","df200k = pd.read_excel('https://drive.google.com/uc?export=download&id=0B-7yvzlCWT6rU0RVLWgxTG9sM0dveWVINEtPMDFHcU1BT0pF',header = 0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bDxMgaoD4_yz","colab_type":"code","colab":{}},"source":["import pandas as pd\n","data = [df50k  ,df50k60k ,df60k70k ,df70k80k,df80k90k ,df90k100k,df100k110k,df110k120k ,df130k140k ,\n","       df140k150k ,df150k160k,df160k170k,df170k180k ,df180k190k ,df190k200k,df200k ]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JK2GPecdSFMs","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gkp7DtFvBi5W","colab_type":"code","colab":{}},"source":["df1 = pd.concat(data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gv2OTgnGzroX","colab_type":"code","colab":{}},"source":["df1.reset_index(drop = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cc1UQXO6YDOx","colab_type":"code","colab":{}},"source":["df1 = df1.groupby('target').tail(2000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VdmyqucV0YS1","colab_type":"code","colab":{}},"source":["df1 = df1.reset_index(drop = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"if7uo8rbBuE9","colab_type":"code","colab":{}},"source":["df1.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mrDEP9UXZzao","colab_type":"code","colab":{}},"source":["pip install selenium"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ypTYNHT38_PW","colab_type":"code","colab":{}},"source":["import nltk\n","nltk.download(\"popular\")\n","nltk.download('wordnet')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"avPAy0QrAfjx","colab_type":"code","colab":{}},"source":["# System\n","import datetime\n","import traceback\n","import ast\n","import os\n","# import datetime\n","from collections import Counter\n","import math\n","# ML\n","import numpy as np\n","import pandas as pd\n","import pickle\n","# WEB\n","import urllib.request\n","from urllib.request import urlopen\n","from bs4 import BeautifulSoup\n","# NLTK\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import RegexpTokenizer\n","# Selenium\n","from selenium import webdriver\n","from selenium.webdriver.common.keys import Keys\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as ec\n","from selenium.webdriver.common.by import By\n","\n","def remove_stopwords(tokens):\n","    for i, token in enumerate(tokens):\n","        tokens[i] = ''.join([ch for ch in token if ch not in char_blacklist])\n","    tokens_sw = [w.lower() for w in tokens if w not in stopwords]\n","    tokens_lemmatize = [wnl.lemmatize(token) for token in tokens_sw]\n","    return tokens_lemmatize\n","\n","def get_en_words(tokens_lemmatize):\n","    english_tokens = []\n","    for word in tokens_lemmatize:\n","        english_tokens.append(word) if word in english_vocab else ''\n","    english_confidence = round(len(english_tokens) / len(tokens_lemmatize) * 100, 2) if len(english_tokens) > 0 else 0\n","    return english_tokens, english_confidence\n","\n","def translate_words():\n","    foreign_words = list(set(tokens_lemmatize) - set(en_tokens))\n","    translated_words = []\n","    if len(foreign_words):\n","        chunk_size = math.ceil(len(foreign_words) / 5000)\n","        chunks = np.array_split(foreign_words, chunk_size)\n","        for chunk in chunks:\n","            foreign_text = \" \".join(chunk)\n","            input_box = driver.find_element_by_id('source')\n","            driver.execute_script(f\"document.getElementById('source').value = '{foreign_text}';\")\n","            # input_box.send_keys(foreign_text)\n","            WebDriverWait(driver, 20).until(ec.visibility_of_element_located((By.CSS_SELECTOR, \"span.tlid-translation.translation\")))\n","            output_box = driver.find_element_by_css_selector(\"span.tlid-translation.translation\").text\n","            translated_words.extend(output_box.split(' '))\n","            input_box.clear()\n","    return translated_words\n","\n","start = datetime.datetime.now()\n","print(start)\n","\n","date = datetime.datetime.today().strftime(\"%Y-%m-%d\")\n","#input_path = f'Datasets/Feature_dataset_{date}.csv'\n","#output_path = f'Datasets/Translated_tokens_{date}.csv'\n","#words_filename = f\"Frequency_models/word_frequency_{date}.picle\"\n","# install chromium, its driver, and selenium\n","!apt-get update\n","!apt install chromium-chromedriver\n","!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n","!pip install selenium\n","# set options to be headless,\n","from selenium import webdriver\n","options = webdriver.ChromeOptions()\n","options.add_argument('--headless')\n","options.add_argument('--no-sandbox')\n","options.add_argument('--disable-dev-shm-usage')\n","# open it, go to a website, and get results\n","driver = webdriver.Chrome('chromedriver',options=options)\n","nltk.download('stopwords')\n","nltk.download('words')\n","#options = webdriver.ChromeOptions()\n","#options.add_argument('headless')\n","driver.get(\"https://translate.google.com/\")\n","\n","english_vocab = set(w.lower() for w in nltk.corpus.words.words('en'))\n","english_tolerance = 50\n","english_confidence = []\n","\n","char_blacklist = list(chr(i) for i in range(32, 127) if (i <= 47 or i >= 58)\\\n","                          and (i <= 64 or i >= 91) and (i <= 96 or i >= 123))\n","stopwords = nltk.corpus.stopwords.words('english')\n","stopwords.extend(char_blacklist)\n","top = 2500\n","hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n","           'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n","           'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n","           'Accept-Encoding': 'none',\n","           'Accept-Language': 'en-US,en;q=0.8',\n","           'Connection': 'keep-alive'}\n","df1 = df1[~df1['tokens'].isnull()]\n","df = df1\n","for row_id, row in df.iterrows():\n"," #print(row['tokens'])\n"," try:\n","  row['tokens'] = ast.literal_eval(row['tokens'])\n","  #row['tokens'] = row['tokens'].tolist()\n","  df.at[row_id,'tokens'] = row['tokens']\n","  #print(row['tokens'])\n"," except :\n","   \n","   print('dropped')\n","   df.drop(row_id, inplace=True)\n","   continue\n","df = df[~df['tokens'].isnull()]\n","\n","\n","df['tokens_en'] = ''\n","df['en_confidence'] = ''\n","counter = 0\n","for row_id, row in df.iterrows():\n","        counter +=1\n","        try:\n","            wnl = WordNetLemmatizer()\n","            tokens_lemmatize = remove_stopwords(row['tokens'])\n","            en_tokens, en_confidence = get_en_words(tokens_lemmatize)\n","            translated_words = translate_words()\n","            en_tokens_tr, en_confidence_tr = get_en_words(translated_words)\n","            en_tokens.extend(remove_stopwords(en_tokens_tr))\n","        \n","            df.at[row_id, 'tokens_en'] = en_tokens if len(en_tokens) else ''\n","            df.at[row_id, 'en_confidence'] = round(len(en_tokens) / len(tokens_lemmatize) * 100, 2) if len(en_tokens) > 0 else 0\n","            print(f\"{counter}/{df.shape[0]} || {row['url']}\")\n","        except Exception:\n","            print(f\"{counter}/{df.shape[0]} || FAILED. {row['url']}\")\n","    #         print(traceback.print_exc())\n","            continue\n","\n","\n","driver.close()\n","stop = datetime.datetime.now()\n","exec_time = stop - start\n","\n","print(exec_time)\n","df = df[df['tokens_en'] != '']\n","#    df.to_csv(output_path, index=False)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ysosbIvwL_Oi","colab_type":"code","colab":{}},"source":["df220 = df\n","data = [df110,df220]\n","df = pd.concat(data)\n","df = df.reset_index(drop = True)\n","df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0gpsoG5uUOsF","colab_type":"code","colab":{}},"source":["from google.colab import files\n","df.to_excel('df.xlsx',index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KESgADGkUUfQ","colab_type":"code","colab":{}},"source":["from google.colab import files\n","files.download('df.xlsx')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7U_F7fmiahD-","colab_type":"code","colab":{}},"source":["#https://drive.google.com/file/d/1Z9xt37_GHegpSi6rR4T6zrbTD5aOy-Bq/view?usp=sharing\n","df = pd.read_excel('https://drive.google.com/uc?export=download&id=1Z9xt37_GHegpSi6rR4T6zrbTD5aOy-Bq',header = 0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YPFDLLMT_jq1","colab_type":"code","colab":{}},"source":["words_frequency = {}\n","for category in set(df['target'].values):\n","        print(category)\n","        all_words = []\n","        for row in df[df['target'] == category]['tokens_en']:\n","            for word in row:\n","                all_words.append(word)\n","        most_common = nltk.FreqDist(w for w in all_words).most_common(top)\n","        words_frequency[category] = most_common\n","# Extract only words\n","for category in set(df['target'].values):\n","        words_frequency[category] = [word for word, number in words_frequency[category]]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XS08A2CQNzz0","colab_type":"code","colab":{}},"source":["words_frequency"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oOe08g1zN9qa","colab_type":"code","colab":{}},"source":["# Save words_frequency model\n","import pickle\n","import os\n","pickle_out = open('data.pkl',\"wb\")\n","pickle.dump(words_frequency, pickle_out)\n","pickle_out.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"__1Qo0ZoROfE","colab_type":"code","colab":{}},"source":["from google.colab import files\n","files.download('data2000.pkl')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"390va5i1RcGh","colab_type":"code","colab":{}},"source":["# To Mount\n","from google.colab import drive\n","drive.mount('drive')\n","# To save on drive\n","!cp data2000.pkl.pkl drive/My\\ Drive/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i7h4kVT_ZoAj","colab_type":"code","colab":{}},"source":["pickle_in = open('data.pkl',\"rb\")\n","words_frequency = pickle.load(pickle_in)\n","    # Models creation\n","top = 2500\n","from collections import Counter\n","\n","features = np.zeros(df.shape[0] * top).reshape(df.shape[0], top)\n","labels = np.zeros(df.shape[0])\n","counter = 0\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g_7NteQ4gO-I","colab_type":"code","colab":{}},"source":["counter = 0\n","for i, row in df.iterrows():\n","        c = [word for word, word_count in Counter(row['tokens_en']).most_common(top)]\n","        labels[counter] = list(set(df['target'].values)).index(row['target'])\n","        for word in c:\n","            if word in words_frequency[row['target']]:\n","                features[counter][words_frequency[row['target']].index(word)] = 1\n","        counter += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wmuCW5dYW9VI","colab_type":"code","colab":{}},"source":["df"],"execution_count":0,"outputs":[]}]}